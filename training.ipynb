{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T10:08:29.675804Z",
     "start_time": "2024-05-15T10:08:29.652207Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.dataloaders_and_sets.simple_dataset import SimpleDataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics.functional import r2_score\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from src.models.fc import VAE\n",
    "from torch.nn.functional import mse_loss\n",
    "from src.training import variational_train\n",
    "from src.data_visualization import pairwise_comparison"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading Data and Preprocessing",
   "id": "98043bbe3bb37de3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:09:04.670003Z",
     "start_time": "2024-05-15T10:08:34.340642Z"
    }
   },
   "cell_type": "code",
   "source": "data_with_targets = pd.read_csv('data/data.csv', index_col=0)",
   "id": "f0fc869bc5c0b9a7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:09:05.067286Z",
     "start_time": "2024-05-15T10:09:04.670799Z"
    }
   },
   "cell_type": "code",
   "source": "data = data_with_targets.fillna(0.0)",
   "id": "6b1bab6cc3ea871f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:20:30.394391Z",
     "start_time": "2024-05-15T10:20:29.568091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove the possible y labels:\n",
    "y_labels = ['primary_disease', 'gender', 'age', 'dataset']\n",
    "data_columns = [col for col in data.columns if col not in y_labels]\n",
    "y = \"gender\"\n",
    "# TODO: makes no sense when using a autoencoder...\n",
    "X_train, X_val, y_train, y_val = train_test_split(data[data_columns], data[y_labels], train_size=0.8, random_state=42, shuffle=True)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ],
   "id": "baa476cf8c63d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9424, 17137) (9424, 4) (2356, 17137) (2356, 4)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:17:05.325109Z",
     "start_time": "2024-05-15T10:17:05.302028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transform dataset for the simple autoencoder\n",
    "\n",
    "transform_fc_ae = {\n",
    "    \"z_score\": \"per_sample\",\n",
    "    \"most_variant\": 5000,\n",
    "}"
   ],
   "id": "f3842da979bd0663",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:09:06.184307Z",
     "start_time": "2024-05-15T10:09:06.164402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if transform_fc_ae.get(\"most_variant\") is not None:\n",
    "    input_dim = transform_fc_ae.get(\"most_variant\")\n",
    "else: \n",
    "    input_dim = X_train.shape()[1]\n"
   ],
   "id": "4bf0bd6186ef22aa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:09:07.498100Z",
     "start_time": "2024-05-15T10:09:06.184938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"{} is used\".format(device))"
   ],
   "id": "f65f7c5de66e595f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup Tensorboard",
   "id": "799aed9d1eef3b47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:48:46.090580Z",
     "start_time": "2024-05-14T07:48:46.067502Z"
    }
   },
   "cell_type": "code",
   "source": "writer = SummaryWriter(\"logs/autoencoder\")\n",
   "id": "846263839add2d82",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:48:46.112406Z",
     "start_time": "2024-05-14T07:48:46.091825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_one_dim = 512\n",
    "hidden_two_dim = 128\n",
    "z_dim = 64\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "learning_rate = 3e-3\n",
    "beta = 0.1\n",
    "print(input_dim)"
   ],
   "id": "bc03513e838236c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "run `tensorboard --logdir=./`",
   "id": "5bd19b620626eeaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "69de9d148b77cd90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple autoencoder",
   "id": "7b40029e6c041f77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:23:19.059536Z",
     "start_time": "2024-05-14T07:23:16.760315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.models.fc import AE\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 6e-4\n",
    "num_epochs = 200\n",
    "\n",
    "hidden_one_dim = 512\n",
    "hidden_two_dim = 128\n",
    "z_dim = 64\n",
    "\n",
    "\n",
    "ae_dataset = SimpleDataset(X_train, transform=transform_fc_ae)\n",
    "ae_dataloader = DataLoader(ae_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "ae = AE(input_size=input_dim, hidden_one_size=hidden_one_dim, hidden_two_size=hidden_two_dim, z_size=z_dim).to(device)\n",
    "optimizer = optim.Adam(ae.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.992)\n",
    "loss_mse = nn.MSELoss()\n",
    "\n",
    "ae_val_dataset = SimpleDataset(X_val, transform=transform_fc_ae)\n",
    "ae_val_dataloader = DataLoader(ae_val_dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "id": "737f6a7b91fce572",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m ae_dataset \u001B[38;5;241m=\u001B[39m SimpleDataset(X_train, transform\u001B[38;5;241m=\u001B[39mtransform_fc_ae)\n\u001B[1;32m     13\u001B[0m ae_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(ae_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m ae \u001B[38;5;241m=\u001B[39m AE(input_size\u001B[38;5;241m=\u001B[39minput_dim, hidden_one_size\u001B[38;5;241m=\u001B[39mhidden_one_dim, hidden_two_size\u001B[38;5;241m=\u001B[39mhidden_two_dim, z_size\u001B[38;5;241m=\u001B[39mz_dim)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     15\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(ae\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlearning_rate)\n\u001B[1;32m     16\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m ExponentialLR(optimizer, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.992\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:23:19.060132Z",
     "start_time": "2024-05-14T07:23:19.060070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ae.eval()\n",
    "writer.add_graph(ae, ae_dataset[0].to(device))\n",
    "writer.close()\n"
   ],
   "id": "3e7fb58a658bb3e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tensorboard logging\n",
    "log_dir = \"\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "additional_comment = \"just for checking what happens if\"\n",
    "writer = SummaryWriter(f\"logs/autoencoder/{log_dir}\", comment=f\"Architecture=({input_dim}-{hidden_one_dim}-{hidden_two_dim}-{z_dim}), learning_rate={learning_rate}, batch_size={batch_size}, number epochs={num_epochs}\" + additional_comment)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ae.train()\n",
    "        \n",
    "    for train_iteration, batch in enumerate(tqdm(ae_dataloader, desc=\"Training epoch {}\".format(epoch+1))):\n",
    "        batch = batch.to(device)\n",
    "        x_reconstructed = ae(batch)\n",
    "        loss = loss_mse(batch, x_reconstructed)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"ae train loss\", loss.item(), epoch * len(ae_dataloader) + train_iteration)\n",
    "        score = r2_score(x_reconstructed, batch).item()\n",
    "        writer.add_scalar(\"ae R2 Score\", score, epoch * len(ae_dataloader) + train_iteration)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_iteration, batch in enumerate(tqdm(ae_val_dataloader, desc=\"Validation epoch {}\".format(epoch+1))):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            x_reconstructed = ae(batch)\n",
    "\n",
    "            score = r2_score(x_reconstructed, batch).item()\n",
    "\n",
    "            writer.add_scalar(\"ae validation R2Score\", score, epoch * len(ae_val_dataloader) + val_iteration)\n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ],
   "id": "4834e4d8df514088",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Variational Autoencoder",
   "id": "449cd05ca2c6605"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:21:57.909819Z",
     "start_time": "2024-05-15T10:21:54.616193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "learning_rate = 6e-4\n",
    "num_epochs = 25\n",
    "beta = 0\n",
    "\n",
    "hidden_one_dim = 512\n",
    "hidden_two_dim = 128\n",
    "z_dim = 64\n",
    "\n",
    "\n",
    "vae_dataset = SimpleDataset(X_train, transform=transform_fc_ae)\n",
    "vae_dataloader = DataLoader(vae_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "vae = VAE(input_size=input_dim, hidden_one_size=hidden_one_dim, hidden_two_size=hidden_two_dim, z_size=z_dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "vae_val_dataset = SimpleDataset(X_val, transform=transform_fc_ae)\n",
    "vae_val_dataloader = DataLoader(vae_val_dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "id": "d6d92da9194cea7d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T10:23:32.880252Z",
     "start_time": "2024-05-15T10:21:57.910814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tensorboard logging\n",
    "log_dir = \"\" + datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "additional_comment = \"\"\n",
    "writer = SummaryWriter(f\"logs/variational autoencoder/{log_dir}_train\", comment=f\"Architecture=({input_dim}-{hidden_one_dim}-{hidden_two_dim}-{z_dim}), learning_rate={learning_rate}, batch_size={batch_size}, number epochs={num_epochs}, beta={beta}\" + additional_comment)\n",
    "\n",
    "writer_val = SummaryWriter(f\"logs/variational autoencoder/{log_dir}_validation\", comment=f\"Architecture=({input_dim}-{hidden_one_dim}-{hidden_two_dim}-{z_dim}), learning_rate={learning_rate}, batch_size={batch_size}, number epochs={num_epochs}\" + additional_comment)\n",
    "vae.eval()\n",
    "writer.add_graph(vae, vae_dataset[0].to(device))\n",
    "writer.close()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    variational_train(vae, vae_dataloader, vae_val_dataloader, optimizer, scheduler, writer, writer_val, epoch, device, beta=beta)"
   ],
   "id": "19bb501eedb93bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fes/Nextcloud/Uni/B.Sc. Bioinfo/Bachelorarbeit/thesis/src/dataloaders_and_sets/simple_dataset.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return torch.tensor(self.data.iloc[index], dtype=torch.float32)\n",
      "/home/fes/anaconda3/envs/Helmholtz/lib/python3.11/site-packages/torch/jit/_trace.py:1116: TracerWarning: Trace had nondeterministic nodes. Did you forget call .eval() on your model? Nodes:\n",
      "\t%eps : Float(64, strides=[1], requires_grad=0, device=cuda:0) = aten::rand_like(%std, %66, %67, %68, %69, %70) # /home/fes/Nextcloud/Uni/B.Sc. Bioinfo/Bachelorarbeit/thesis/src/models/fc.py:32:0\n",
      "This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n",
      "  _check_trace(\n",
      "/home/fes/anaconda3/envs/Helmholtz/lib/python3.11/site-packages/torch/jit/_trace.py:1116: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 4996 / 5000 (99.9%)\n",
      "Greatest absolute difference: 0.12666283547878265 at index (3787,) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 2732.602014173816 at index (956,) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "Training epoch 1: 100%|██████████| 148/148 [00:02<00:00, 66.63it/s] \n",
      "Validation epoch 1: 100%|██████████| 37/37 [00:01<00:00, 26.34it/s]\n",
      "Training epoch 2: 100%|██████████| 148/148 [00:02<00:00, 70.59it/s]\n",
      "Validation epoch 2: 100%|██████████| 37/37 [00:01<00:00, 23.53it/s]\n",
      "Training epoch 3: 100%|██████████| 148/148 [00:01<00:00, 80.29it/s]\n",
      "Validation epoch 3: 100%|██████████| 37/37 [00:01<00:00, 27.75it/s]\n",
      "Training epoch 4: 100%|██████████| 148/148 [00:02<00:00, 63.52it/s]\n",
      "Validation epoch 4: 100%|██████████| 37/37 [00:01<00:00, 25.29it/s]\n",
      "Training epoch 5: 100%|██████████| 148/148 [00:02<00:00, 66.67it/s]\n",
      "Validation epoch 5: 100%|██████████| 37/37 [00:01<00:00, 26.12it/s]\n",
      "Training epoch 6: 100%|██████████| 148/148 [00:02<00:00, 64.21it/s]\n",
      "Validation epoch 6: 100%|██████████| 37/37 [00:01<00:00, 26.74it/s]\n",
      "Training epoch 7: 100%|██████████| 148/148 [00:02<00:00, 69.75it/s]\n",
      "Validation epoch 7: 100%|██████████| 37/37 [00:01<00:00, 27.56it/s]\n",
      "Training epoch 8: 100%|██████████| 148/148 [00:02<00:00, 67.45it/s] \n",
      "Validation epoch 8: 100%|██████████| 37/37 [00:01<00:00, 26.90it/s]\n",
      "Training epoch 9: 100%|██████████| 148/148 [00:02<00:00, 62.43it/s]\n",
      "Validation epoch 9: 100%|██████████| 37/37 [00:01<00:00, 24.61it/s]\n",
      "Training epoch 10: 100%|██████████| 148/148 [00:02<00:00, 65.97it/s]\n",
      "Validation epoch 10: 100%|██████████| 37/37 [00:01<00:00, 25.23it/s]\n",
      "Training epoch 11: 100%|██████████| 148/148 [00:02<00:00, 59.90it/s]\n",
      "Validation epoch 11: 100%|██████████| 37/37 [00:01<00:00, 22.45it/s]\n",
      "Training epoch 12: 100%|██████████| 148/148 [00:02<00:00, 55.30it/s]\n",
      "Validation epoch 12: 100%|██████████| 37/37 [00:01<00:00, 25.76it/s]\n",
      "Training epoch 13: 100%|██████████| 148/148 [00:02<00:00, 60.90it/s]\n",
      "Validation epoch 13: 100%|██████████| 37/37 [00:01<00:00, 24.91it/s]\n",
      "Training epoch 14: 100%|██████████| 148/148 [00:02<00:00, 64.94it/s]\n",
      "Validation epoch 14: 100%|██████████| 37/37 [00:01<00:00, 23.99it/s]\n",
      "Training epoch 15: 100%|██████████| 148/148 [00:02<00:00, 61.32it/s]\n",
      "Validation epoch 15: 100%|██████████| 37/37 [00:01<00:00, 24.69it/s]\n",
      "Training epoch 16: 100%|██████████| 148/148 [00:02<00:00, 54.25it/s]\n",
      "Validation epoch 16: 100%|██████████| 37/37 [00:01<00:00, 24.42it/s]\n",
      "Training epoch 17: 100%|██████████| 148/148 [00:02<00:00, 62.42it/s]\n",
      "Validation epoch 17: 100%|██████████| 37/37 [00:01<00:00, 24.84it/s]\n",
      "Training epoch 18: 100%|██████████| 148/148 [00:02<00:00, 61.71it/s]\n",
      "Validation epoch 18: 100%|██████████| 37/37 [00:01<00:00, 24.89it/s]\n",
      "Training epoch 19: 100%|██████████| 148/148 [00:02<00:00, 65.39it/s]\n",
      "Validation epoch 19: 100%|██████████| 37/37 [00:01<00:00, 24.65it/s]\n",
      "Training epoch 20: 100%|██████████| 148/148 [00:02<00:00, 57.61it/s]\n",
      "Validation epoch 20: 100%|██████████| 37/37 [00:01<00:00, 25.50it/s]\n",
      "Training epoch 21: 100%|██████████| 148/148 [00:02<00:00, 67.72it/s]\n",
      "Validation epoch 21: 100%|██████████| 37/37 [00:01<00:00, 26.02it/s]\n",
      "Training epoch 22: 100%|██████████| 148/148 [00:02<00:00, 65.37it/s]\n",
      "Validation epoch 22: 100%|██████████| 37/37 [00:01<00:00, 25.26it/s]\n",
      "Training epoch 23: 100%|██████████| 148/148 [00:02<00:00, 59.82it/s]\n",
      "Validation epoch 23: 100%|██████████| 37/37 [00:01<00:00, 24.20it/s]\n",
      "Training epoch 24: 100%|██████████| 148/148 [00:02<00:00, 63.84it/s]\n",
      "Validation epoch 24: 100%|██████████| 37/37 [00:01<00:00, 25.70it/s]\n",
      "Training epoch 25: 100%|██████████| 148/148 [00:02<00:00, 64.61it/s]\n",
      "Validation epoch 25: 100%|██████████| 37/37 [00:01<00:00, 23.57it/s]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:48:19.744942Z",
     "start_time": "2024-05-14T12:48:19.721819Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Architecture=({input_dim}-{hidden_one_dim}-{hidden_two_dim}-{z_dim}), learning_rate={learning_rate}, batch_size={batch_size}, number epochs={num_epochs}\" + additional_comment)",
   "id": "41045290ee5ec67e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture=(5000-512-128-64), learning_rate=0.0006, batch_size=512, number epochs=25just for checking what happens if\n"
     ]
    }
   ],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
